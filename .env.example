# LLM Service Configuration

# Server Configuration
PORT=8000
HOST=0.0.0.0
LOG_LEVEL=INFO
RELOAD=false

# Auto-shutdown Configuration
SHUTDOWN_TIMEOUT=600  # Shutdown after 10 minutes of inactivity (0 to disable)

# Cache Configuration
ENABLE_CACHE=true
CACHE_MAX_SIZE=1000
CACHE_TTL_SECONDS=3600
CACHE_PERSIST_TO_DISK=false
CACHE_DIR=.cache/llm_responses

# Model Configuration
# Each model tier (LIGHT, MEDIUM, HEAVY) can be configured independently
# Supported backends: llamacpp (for .gguf files), mlx (for MLX models)

# LIGHT Model Configuration
LIGHT_MODEL_PATH=/path/to/light-model.gguf
LIGHT_BACKEND=llamacpp
LIGHT_N_CTX=8192
LIGHT_N_BATCH=512
LIGHT_N_THREADS=8
LIGHT_N_GPU_LAYERS=-1  # -1 for all layers on GPU
LIGHT_TEMPERATURE=0.1
LIGHT_TOP_P=0.95
LIGHT_TOP_K=40
LIGHT_MAX_TOKENS=1000
LIGHT_REPEAT_PENALTY=1.0
LIGHT_USE_MMAP=true
LIGHT_USE_MLOCK=false

# MEDIUM Model Configuration
MEDIUM_MODEL_PATH=/path/to/medium-model.gguf
MEDIUM_BACKEND=llamacpp
MEDIUM_N_CTX=16384
MEDIUM_N_BATCH=512
MEDIUM_N_THREADS=8
MEDIUM_N_GPU_LAYERS=-1
MEDIUM_TEMPERATURE=0.3
MEDIUM_TOP_P=0.95
MEDIUM_TOP_K=40
MEDIUM_MAX_TOKENS=2048
MEDIUM_REPEAT_PENALTY=1.1
MEDIUM_USE_MMAP=true
MEDIUM_USE_MLOCK=false

# HEAVY Model Configuration
HEAVY_MODEL_PATH=/path/to/heavy-model.gguf
HEAVY_BACKEND=llamacpp
HEAVY_N_CTX=32768
HEAVY_N_BATCH=512
HEAVY_N_THREADS=8
HEAVY_N_GPU_LAYERS=-1
HEAVY_TEMPERATURE=0.7
HEAVY_TOP_P=0.95
HEAVY_TOP_K=40
HEAVY_MAX_TOKENS=4096
HEAVY_REPEAT_PENALTY=1.1
HEAVY_USE_MMAP=true
HEAVY_USE_MLOCK=false

# Example MLX Model Configuration
# LIGHT_MODEL_PATH=/path/to/mlx-model-directory
# LIGHT_BACKEND=mlx
# LIGHT_N_CTX=8192
# LIGHT_TEMPERATURE=0.1
# LIGHT_TOP_P=0.95
# LIGHT_MAX_TOKENS=1000
# LIGHT_REPEAT_PENALTY=1.0

# Model Path Examples:
# Llama.cpp models (GGUF format):
# - Qwen 0.5B: ~/.cache/lm-studio/models/Qwen2.5-0.5B-Instruct-GGUF/Qwen2.5-0.5B-Instruct-Q8_0.gguf
# - Phi 3 Mini: ~/.cache/lm-studio/models/microsoft/Phi-3-mini-4k-instruct-gguf/Phi-3-mini-4k-instruct-Q4.gguf
# - Gemma 2B: ~/.cache/lm-studio/models/gemma-2b-it-GGUF/gemma-2b-it-Q4_K_M.gguf
# - Mistral 7B: ~/.cache/lm-studio/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf
#
# MLX models (directory format):
# - Qwen 0.5B: ~/.cache/mlx_models/mlx-community/Qwen2.5-0.5B-Instruct-4bit
# - Phi 3 Mini: ~/.cache/mlx_models/mlx-community/phi-3-mini-4k-instruct-4bit
# - Gemma 2B: ~/.cache/mlx_models/mlx-community/gemma-2b-it-4bit

# EMBEDDING Model Configuration (for BGE-M3 or other embedding models)
# Option 1: Using GGUF format with llama.cpp
EMBEDDING_MODEL_PATH=/path/to/bge-m3-Q8_0.gguf
EMBEDDING_BACKEND=llamacpp
EMBEDDING_N_CTX=8192  # BGE-M3 supports up to 8192 tokens
EMBEDDING_N_BATCH=2048  # Larger batch for embedding throughput
EMBEDDING_N_THREADS=8
EMBEDDING_N_GPU_LAYERS=-1
EMBEDDING_TEMPERATURE=0.1  # Not used for embeddings, but required
EMBEDDING_TOP_P=0.95
EMBEDDING_TOP_K=40
EMBEDDING_MAX_TOKENS=1000
EMBEDDING_REPEAT_PENALTY=1.0
EMBEDDING_USE_MMAP=true
EMBEDDING_USE_MLOCK=false
EMBEDDING_EMBEDDING_MODE=true  # Mark this as an embedding model
EMBEDDING_EMBEDDING_DIMENSION=1024  # BGE-M3 has 1024 dimensions

# Option 2: Using MLX format (for Apple Silicon)
# EMBEDDING_MODEL_PATH=/path/to/mlx-community/bge-m3-mlx
# EMBEDDING_BACKEND=mlx
# EMBEDDING_N_CTX=8192
# EMBEDDING_TEMPERATURE=0.1
# EMBEDDING_EMBEDDING_MODE=true
# EMBEDDING_EMBEDDING_DIMENSION=1024

# RERANKER Model Configuration (for BGE-reranker or other cross-encoder models)
# RERANKER_MODEL_PATH=/path/to/bge-reranker-v2-m3-Q8_0.gguf
# RERANKER_BACKEND=llamacpp
# RERANKER_N_CTX=512  # Rerankers typically need less context
# RERANKER_N_BATCH=512
# RERANKER_N_THREADS=8
# RERANKER_N_GPU_LAYERS=-1
# RERANKER_TEMPERATURE=0.01  # Very low temperature for consistent scoring
# RERANKER_TOP_P=0.95
# RERANKER_TOP_K=40
# RERANKER_MAX_TOKENS=1
# RERANKER_REPEAT_PENALTY=1.0
# RERANKER_USE_MMAP=true
# RERANKER_USE_MLOCK=false